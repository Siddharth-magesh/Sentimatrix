{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Siddharth\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import  login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Overall Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary:**\n",
      "\n",
      "The overall sentiment of the reviews is mostly positive, with customers praising the dishwasher's efficiency, quiet operation, and cleaning performance. Key pros mentioned include:\n",
      "\n",
      "* Quiet and efficient operation\n",
      "* Effective cleaning and removal of food residue\n",
      "* Ample capacity and adjustable racks\n",
      "* Energy efficiency and minimal water usage\n",
      "* Simple controls and easy operation\n",
      "\n",
      "Notable drawbacks mentioned include:\n",
      "\n",
      "* Some users find the cycles to be too long\n",
      "* Drying performance could be improved\n",
      "* Build quality is not exceptional, especially for the price\n",
      "* Some users experienced issues with the top rack holding cups securely\n",
      "\n",
      "One reviewer reported a disappointing experience with the dishwasher stopping working after just three months, with poor customer service.\n",
      "\n",
      "**Overall Rating:** 4.2/5\n",
      "\n",
      "The majority of reviewers are satisfied with their purchase, citing its effectiveness, efficiency, and value for the price. However, some users have noted some drawbacks, particularly with drying performance and build quality.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=KEY\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a review summarizer AI. Your task is to analyze and summarize customer feedback on a particular product. You will be given a set of reviews and need to extract the key points, including the main pros and cons mentioned by customers. Provide a concise summary that highlights the overall sentiment, key positive aspects, and notable drawbacks.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Summarize the Follwing Reviews\n",
    "\n",
    "            1. Review 1: *\"This dishwasher is super quiet and efficient. My dishes come out spotless every time. Installation was a bit tricky, but worth it!\"*\n",
    "\n",
    "            2. Review 2: *\"I’ve had this dishwasher for 6 months now, and it works great! It handles even the dirtiest dishes and leaves no residue.\"*\n",
    "\n",
    "            3. Review 3: *\"The capacity is amazing. It fits more dishes than my old one, and I love the different washing modes. However, drying could be better.\"*\n",
    "\n",
    "            4. Review 4: *\"Decent dishwasher, but I wish the cycles were shorter. It cleans well but takes a long time to complete a load.\"*\n",
    "\n",
    "            5. Review 5: *\"Not impressed. The top rack doesn’t hold cups securely, and the build quality feels a bit cheap for the price.\"*\n",
    "\n",
    "            6. Review 6: *\"Very energy efficient! My utility bill has gone down, and it does a fantastic job with minimal water usage.\"*\n",
    "\n",
    "            7. Review 7: *\"Easy to operate with simple controls. The noise level is low, which is perfect for my open kitchen design.\"*\n",
    "\n",
    "            8. Review 8: *\"Unfortunately, it stopped working after just 3 months. Customer service was not very helpful either. Disappointed.\"*\n",
    "\n",
    "            9. Review 9: *\"It has all the features I need, like adjustable racks and multiple cycles. My dishes always come out shiny and clean.\"*\n",
    "\n",
    "            10. Review 10: *\"Great value for the price! It’s not the fanciest model, but it does exactly what I need. No issues so far.\"*\n",
    "            \"\"\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.5,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis for each product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\newenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "d:\\newenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"This dishwasher is super quiet and efficient. My dishes come out spotless every time. Installation was a bit tricky, but worth it!\",\n",
    "    \"I’ve had this dishwasher for 6 months now, and it works great! It handles even the dirtiest dishes and leaves no residue.\",\n",
    "    \"The capacity is amazing. It fits more dishes than my old one, and I love the different washing modes. However, drying could be better.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9676889181137085}, {'label': 'positive', 'score': 0.9711691737174988}, {'label': 'positive', 'score': 0.9588515758514404}]\n"
     ]
    }
   ],
   "source": [
    "result_arr = []\n",
    "for review in reviews:\n",
    "    sent = pipe(review)\n",
    "    result_arr.append(sent[0])\n",
    "print(result_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.8907274007797241}]\n"
     ]
    }
   ],
   "source": [
    "x = pipe(\"This is totally waste of money\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Particular customer Feedback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the customer review and sentiment analysis, the customer feels positively about the product. They mention that the \"build quality, RGB lights, and performance\" are \"pretty good\". The only negative comment is that the price seems \"too expensive\", but they still recommend buying the product, suggesting that the benefits outweigh the cost.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=KEY)\n",
    "\n",
    "# Define the sentiment and question dynamically\n",
    "sentiment = [{'label': 'positive', 'score': 0.6416164040565491}]\n",
    "content = \"Build quality RGB lights and perfomance evrything is pretty good but the price seems too expensive.I RECOMMEND YOU TO BUY THIS PRODUCT ON BLACK FRIDAY for more offers. ITS ON 29 NOVEMBER 2024\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You have given the customer review and sentiment for that comment. How does the customer feel about the product?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"{content}\n",
    "            SENTIMENT: {sentiment}\n",
    "            QUESTION: \"How does the customer feel about the product?\"\"\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the review and sentiment analysis, it appears that the customer has mixed feelings about the product. On the positive side, they mention that the build quality, RGB lights, and performance are \"pretty good\". This suggests that the customer is generally satisfied with the product's functionality and design.\n",
      "\n",
      "However, the customer also expresses a negative sentiment, stating that the price is \"too expensive\". This suggests that the customer feels that the product is overpriced and may not be worth the cost.\n",
      "\n",
      "Despite this, the customer still recommends buying the product, but only on Black Friday when there may be discounts and offers available. This implies that the customer is willing to consider purchasing the product if the price is reduced.\n",
      "\n",
      "Overall, the customer's feelings about the product are complex, with both positive and negative sentiments expressed. While they are generally satisfied with the product's performance and design, they are hesitant to recommend it at the current price point.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=KEY)\n",
    "\n",
    "# Define sentiment and content dynamically\n",
    "sentiment = [{'label': 'positive', 'score': 0.6416164040565491}]\n",
    "content = \"Build quality RGB lights and perfomance evrything is pretty good but the price seems too expensive.I RECOMMEND YOU TO BUY THIS PRODUCT ON BLACK FRIDAY for more offers. ITS ON 29 NOVEMBER 2024\"\n",
    "\n",
    "# Limit content size to prevent API overuse (you can truncate longer content if needed)\n",
    "MAX_LENGTH = 300\n",
    "if len(content) > MAX_LENGTH:\n",
    "    content = content[:MAX_LENGTH] + \"...\"\n",
    "\n",
    "# Create the chat completion\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Given the review and sentiment, provide insights on the customer's feelings about the product.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"REVIEW: {content}\\nSENTIMENT: {sentiment}\\nQUESTION: How does the customer feel about the product?\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    #max_tokens=100,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(chat_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "l.append(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Based on the review and sentiment analysis, it appears that the customer is generally satisfied with the product\\'s build quality, RGB lights, and performance. They mention that \"everything is pretty good\", indicating a positive overall impression.\\n\\nHowever, the customer also expresses a concern about the price, stating that it seems \"too expensive\". This suggests that the customer may be hesitant to purchase the product at its current price point.\\n\\nThe customer does provide a recommendation to buy the product on Black Friday, which implies that they believe the product is worth purchasing, but only at a discounted price. They also specify the date of the sale, indicating that they are keeping an eye out for a good deal.\\n\\nOverall, the customer\\'s feelings about the product are mixed. They are pleased with its quality and performance, but are deterred by the high price.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Langchain Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaForCausalLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "The model 'LlamaForCausalLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "d:\\newenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n",
      "d:\\newenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "d:\\newenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given the following summary:\n",
      "You are given the following input:\n",
      "Describe the impact of AI on the future of work.\n",
      "Generate a brief summary.\n",
      "AI is transforming the way we work, and its impact on the future of work is significant. The rise of automation and machine learning is creating new job opportunities, while also disrupting traditional industries.\n",
      "One of the most significant impacts of AI on the future of work is the rise of automation. Automation is already transforming the manufacturing industry, with robots replacing human workers in factories. This trend is expected to continue, with AI being used to improve production efficiency and reduce costs.\n",
      "Another significant impact of AI on the future of work is the rise of the gig economy. AI is being used to automate tasks such as data analysis, customer service, and logistics. This is creating new job opportunities for freelancers and gig workers, while also reducing the need for traditional office jobs.\n",
      "The rise of AI is also creating new job opportunities in areas such as data analysis, machine learning, and artificial intelligence. These jobs are expected to grow rapidly in the coming years, as companies seek to leverage AI to improve their operations and drive innovation.\n",
      "However, the rise of AI is also creating new challenges for traditional industries. For example, the automotive industry is facing the challenge of automating vehicles, which could lead to job losses for human workers. Similarly, the healthcare industry is facing the challenge of automating medical procedures, which could lead to job losses for medical professionals.\n",
      "Overall, the impact of AI on the future of work is complex and multifaceted. While it is creating new job opportunities, it is also disrupting traditional industries and creating new challenges. As AI continues to evolve, it will be important for companies to adapt and prepare for the future of work.\n",
      "Now, convert this summary into a detailed report.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the Hugging Face pipelines for the LLMs\n",
    "llm1 = pipeline(\"text2text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"cpu\")\n",
    "llm2 = pipeline(\"text2text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"cpu\")\n",
    "# Wrap the pipelines with LangChain's HuggingFacePipeline\n",
    "llm1 = HuggingFacePipeline(pipeline=llm1)\n",
    "llm2 = HuggingFacePipeline(pipeline=llm2)\n",
    "\n",
    "# Define the first LLM prompt template\n",
    "template_1 = \"\"\"You are given the following input:\n",
    "{input_text}\n",
    "Generate a brief summary.\"\"\"\n",
    "\n",
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"input_text\"],\n",
    "    template=template_1,\n",
    ")\n",
    "\n",
    "# Define the second LLM prompt template\n",
    "template_2 = \"\"\"You are given the following summary:\n",
    "{summary}\n",
    "Now, convert this summary into a detailed report.\"\"\"\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=template_2,\n",
    ")\n",
    "\n",
    "# Create two LLMChains\n",
    "chain_1 = LLMChain(llm=llm1, prompt=prompt_1)\n",
    "chain_2 = LLMChain(llm=llm2, prompt=prompt_2)\n",
    "\n",
    "def llm_chain_pipeline(input_text):\n",
    "    # Pass the input to the first chain\n",
    "    summary = chain_1.run({\"input_text\": input_text})\n",
    "    print(\"-----------first----------------\")\n",
    "    print(summary)\n",
    "    \n",
    "    # Pass the output of the first chain as input to the second chain\n",
    "    detailed_report = chain_2.run({\"summary\": summary})\n",
    "    print(\"-----------Second----------------\")\n",
    "    print(detailed_report)\n",
    "    return detailed_report\n",
    "\n",
    "# Test the chain\n",
    "input_text = \"Describe the impact of AI on the future of work.\"\n",
    "result = llm_chain_pipeline(input_text)\n",
    "print(\"-----------Final----------------\")\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
